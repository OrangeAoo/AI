{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taU4ymXoTp4G",
        "outputId": "bbb89e72-e004-462a-eaaf-faf986b32c33"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 512, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_heads, dropout=0.1):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert embedding_dim % num_heads == 0, \"Embedding dimension must be divisible by number of heads\"\n",
        "\n",
        "\n",
        "        self.num_heads = num_heads #NOTE: # of heads\n",
        "        self.head_dim = embedding_dim // num_heads #NOTE: # of dimension per head [batch, seq_l, emb_dim] --> [batch, head, seq_l, head_dim]\n",
        "\n",
        "        # Linear layers for query, key, and value\n",
        "        self.query_linear = nn.Linear(embedding_dim, embedding_dim) #NOTE: Q-projection for the WHOLE input with dim unchanged\n",
        "        self.key_linear = nn.Linear(embedding_dim, embedding_dim) #NOTE: K-projection for the WHOLE input with dim unchanged\n",
        "        self.value_linear = nn.Linear(embedding_dim, embedding_dim) #NOTE: V-projection for the WHOLE input with dim unchanged\n",
        "\n",
        "        # Linear layer for output projection\n",
        "        self.out_linear = nn.Linear(embedding_dim, embedding_dim) #NOTE: for recombining the concatenated heads\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)  #NOTE: for adding some noises --> improve generalizability\n",
        "        self.scale = self.head_dim ** -0.5  # Scaling factor (1 / sqrt(d_k))\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.size(0) #NOTE: get the batch size\n",
        "\n",
        "        # Linear transformations\n",
        "        #NOTE: ---------------------\n",
        "        # Why transforming as a WHOLE before cutting the input?\n",
        "        #     - make sure each head is looking at the SAME Q/K/V but in DIFFERENT perspectives\n",
        "        #       (i.e. mathematically, subspaces/dimentions)\n",
        "        #     - if cut the input first, then each head will be \"blinded\" to look only at a subset of\n",
        "        #       the original data -> losing the holistic meaning.\n",
        "        # ---------------------------\n",
        "        '''#params: embedding_dim * (embedding_dim+1) *3'''\n",
        "        Q = self.query_linear(query)  #NOTE:  (batch_size, seq_len, embedding_dim)\n",
        "        K = self.key_linear(key)      #NOTE:  (batch_size, seq_len, embedding_dim)\n",
        "        V = self.value_linear(value)  #NOTE:  (batch_size, seq_len, embedding_dim)\n",
        "\n",
        "        # Reshape into (batch_size, num_heads, seq_len, head_dim)\n",
        "        #NOTE: [batch, seq_l, emb_dim] --> [batch, head, seq_l, head_dim]\n",
        "        '''#params: 0'''\n",
        "        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        K = K.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        V = V.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        '''#params: 0'''\n",
        "        #NOTE: Compute scaled dot-product attention scores:\n",
        "        #      QK^T gives [batch, head, seq_l, seq_l], scaled by 1/sqrt\n",
        "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
        "        if mask is not None:  #NOTE: apply the causal attention mask to prevent peeking into later words\n",
        "            attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))\n",
        "        #NOTE: normalize using softmax -> score between [0,1]\n",
        "        attention_probs = F.softmax(attention_scores, dim=-1)\n",
        "        attention_probs = self.dropout(attention_probs) #NOTE: add some noise\n",
        "\n",
        "        # Attention output\n",
        "        attention_output = torch.matmul(attention_probs, V)  #NOTE: (batch_size, num_heads, seq_len, head_dim)\n",
        "\n",
        "        #NOTE: directly concatenate heads and put through final linear layer\n",
        "        attention_output = attention_output.transpose(1, 2).contiguous()\n",
        "        attention_output = attention_output.view(batch_size, -1, self.num_heads * self.head_dim)\n",
        "        '''#params: embedding_dim*(embedding_dim+1)'''\n",
        "        output = self.out_linear(attention_output)  #NOTE: for recombining the concatenated heads\n",
        "\n",
        "        return output\n",
        "\n",
        "# Example usage\n",
        "embedding_dim = 768\n",
        "num_heads = 12\n",
        "dropout = 0.1\n",
        "mha = MultiHeadAttention(embedding_dim, num_heads, dropout)\n",
        "\n",
        "# Dummy input\n",
        "batch_size = 32\n",
        "seq_len = 512\n",
        "dummy_input = torch.rand(batch_size, seq_len, embedding_dim)\n",
        "\n",
        "# Forward pass\n",
        "output = mha(dummy_input, dummy_input, dummy_input)\n",
        "output.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Manually"
      ],
      "metadata": {
        "id": "XKK1FLv099fK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnt_params = embedding_dim*(embedding_dim+1)*3 ##NOTE: K/Q/V\n",
        "cnt_params += embedding_dim*(embedding_dim+1) ## output\n",
        "cnt_params\n"
      ],
      "metadata": {
        "id": "2Icd0CABTqMv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f249ef58-30b9-4753-c754-dbfbd76ecd93"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2362368"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## loop"
      ],
      "metadata": {
        "id": "uQODhZTn9_pL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sum([p.numel() for p in mha.parameters() if p.requires_grad])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-H_tJb2533S5",
        "outputId": "6b2bd4b8-949c-42b2-d547-9f126291a0fe"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2362368"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zKM7Dt8W91ys"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}